{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "11511c7e",
      "metadata": {
        "id": "11511c7e"
      },
      "source": [
        "<figure>\n",
        "<img src=\"https://raw.githubusercontent.com/sanchit-gandhi/notebooks/main/whisper_architecture.svg\" alt=\"Trulli\" style=\"width:100%\">\n",
        "<figcaption align = \"center\"><b>Figure 1:</b> Whisper model. The architecture \n",
        "follows the standard Transformer-based encoder-decoder model. A \n",
        "log-Mel spectrogram is input to the encoder. The last encoder \n",
        "hidden states are input to the decoder via cross-attention mechanisms. The \n",
        "decoder autoregressively predicts text tokens, jointly conditional on the \n",
        "encoder hidden states and previously predicted tokens. "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Available models and languages\n",
        "There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and relative speed.\n",
        "\n",
        "| Size   | Parameters | Required VRAM | Relative speed |\n",
        "|--------|------------|---------------|----------------|\n",
        "| tiny   | 39 M       | ~1 GB         | ~32x           |\n",
        "| base   | 74 M       | ~1 GB         | ~16x           |\n",
        "| small  | 244 M      | ~2 GB         | ~6x            |\n",
        "| medium | 769 M      | ~5 GB         | ~2x            |\n",
        "| large  | 1550 M     | ~10 GB        | 1x             |\n",
        "\n",
        "### We will be utilizing a <b> small </b> model for our needs"
      ],
      "metadata": {
        "id": "-BXhcDCpjuji"
      },
      "id": "-BXhcDCpjuji"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Environment"
      ],
      "metadata": {
        "id": "63Pf0q0Hm7sn"
      },
      "id": "63Pf0q0Hm7sn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f9c8fb3",
      "metadata": {
        "id": "8f9c8fb3"
      },
      "outputs": [],
      "source": [
        "#checking GPU stats\n",
        "gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Not connected to a GPU')\n",
        "else:\n",
        "  print(gpu_info)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a3d94b4",
      "metadata": {
        "scrolled": true,
        "id": "9a3d94b4"
      },
      "outputs": [],
      "source": [
        "!pip install datasets>=2.6.1\n",
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install librosa\n",
        "!pip install evaluate>=0.30\n",
        "!pip install jiwer\n",
        "!pip install gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.cuda.is_available()       \n",
        "#must return True"
      ],
      "metadata": {
        "id": "ct3AJY6AtJYM"
      },
      "id": "ct3AJY6AtJYM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transforming Custom Datasets into Model Specification Format"
      ],
      "metadata": {
        "id": "OgbnAw0FnCP_"
      },
      "id": "OgbnAw0FnCP_"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "0437a5ff",
      "metadata": {
        "id": "0437a5ff"
      },
      "outputs": [],
      "source": [
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "from datasets import Audio\n",
        "import gc\n",
        "df = pd.read_csv('data.csv')\n",
        "df.columns = ['audio', 'sentence']\n",
        "train_df=df.iloc[:900,:]\n",
        "test_df=df.iloc[900:,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "49a98f1a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "49a98f1a",
        "outputId": "5f3b6734-d878-41ff-d731-113b2064f200"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                  audio  \\\n",
              "900  C:\\\\Users\\\\test\\\\wav\\\\tel_0902.wav   \n",
              "901  C:\\\\Users\\\\test\\\\wav\\\\tel_0903.wav   \n",
              "902  C:\\\\Users\\\\test\\\\wav\\\\tel_0904.wav   \n",
              "903  C:\\\\Users\\\\test\\\\wav\\\\tel_0905.wav   \n",
              "904  C:\\\\Users\\\\test\\\\wav\\\\tel_0906.wav   \n",
              "\n",
              "                                             sentence  \n",
              "900       ప్రపంచ ప్రఖ్యాత వ్యవసాయ శాస్త్రవేత్త  ఆయన    \n",
              "901                 నమస్కారం మా ఊరి పేరు వికీలో లేదు   \n",
              "902    వచ్చే మూడు నాలుగు నెలలు నాకు పరీక్షలు ఉన్నాయి   \n",
              "903  దిద్దుబాటు చేసిన వెంటనే మార్పులు కనిపించడం లేదు   \n",
              "904         అన్నవరం శ్రీ సత్యనారాయణ స్వామివారి జయంతి   "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-81de97fe-3a40-4da1-b2ad-a4b14e700473\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>audio</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>900</th>\n",
              "      <td>C:\\\\Users\\\\test\\\\wav\\\\tel_0902.wav</td>\n",
              "      <td>ప్రపంచ ప్రఖ్యాత వ్యవసాయ శాస్త్రవేత్త  ఆయన</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>901</th>\n",
              "      <td>C:\\\\Users\\\\test\\\\wav\\\\tel_0903.wav</td>\n",
              "      <td>నమస్కారం మా ఊరి పేరు వికీలో లేదు</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>902</th>\n",
              "      <td>C:\\\\Users\\\\test\\\\wav\\\\tel_0904.wav</td>\n",
              "      <td>వచ్చే మూడు నాలుగు నెలలు నాకు పరీక్షలు ఉన్నాయి</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>903</th>\n",
              "      <td>C:\\\\Users\\\\test\\\\wav\\\\tel_0905.wav</td>\n",
              "      <td>దిద్దుబాటు చేసిన వెంటనే మార్పులు కనిపించడం లేదు</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>904</th>\n",
              "      <td>C:\\\\Users\\\\test\\\\wav\\\\tel_0906.wav</td>\n",
              "      <td>అన్నవరం శ్రీ సత్యనారాయణ స్వామివారి జయంతి</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-81de97fe-3a40-4da1-b2ad-a4b14e700473')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-81de97fe-3a40-4da1-b2ad-a4b14e700473 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-81de97fe-3a40-4da1-b2ad-a4b14e700473');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "test_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b032ae",
      "metadata": {
        "id": "30b032ae"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97e6d988",
      "metadata": {
        "id": "97e6d988"
      },
      "outputs": [],
      "source": [
        "train_dataset = train_dataset.cast_column('audio', Audio(sampling_rate=16000))\n",
        "test_dataset = test_dataset.cast_column('audio', Audio(sampling_rate=16000))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "550f2f96",
      "metadata": {
        "id": "550f2f96",
        "outputId": "c426f07b-da05-42d4-98cb-c3ac368c48a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'audio': {'path': 'C:\\\\\\\\Users\\\\\\\\test\\\\\\\\wav\\\\\\\\tel_0002.wav',\n",
              "  'array': array([-0.00180054, -0.00201416, -0.00192261, ..., -0.00109863,\n",
              "         -0.00125122, -0.00115967], dtype=float32),\n",
              "  'sampling_rate': 16000},\n",
              " 'sentence': 'ఈ గ్రామంలో ప్రజల ప్రధాన వృత్తి వ్యవసాయం '}"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare Feature Extractor, Tokenizer and Data"
      ],
      "metadata": {
        "id": "56dGA_l4pUud"
      },
      "id": "56dGA_l4pUud"
    },
    {
      "cell_type": "markdown",
      "source": [
        "####The ASR pipeline can be de-composed into three stages: \n",
        "1-> A feature extractor which pre-processes the raw audio-inputs\n",
        "\n",
        "2->The model which performs the sequence-to-sequence mapping \n",
        "\n",
        "3->A tokenizer which post-processes the model outputs to text format"
      ],
      "metadata": {
        "id": "rYTsZ7ibpI7n"
      },
      "id": "rYTsZ7ibpI7n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *WhisperFeatureExtractor*"
      ],
      "metadata": {
        "id": "9Lyd0ScqpjxW"
      },
      "id": "9Lyd0ScqpjxW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "818c86e2",
      "metadata": {
        "id": "818c86e2"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperFeatureExtractor\n",
        "\n",
        "feature_extractor = WhisperFeatureExtractor.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *WhisperTokenizer*"
      ],
      "metadata": {
        "id": "81yUGlVZpoLM"
      },
      "id": "81yUGlVZpoLM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08ad09b",
      "metadata": {
        "id": "f08ad09b"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperTokenizer\n",
        "\n",
        "tokenizer = WhisperTokenizer.from_pretrained(\"openai/whisper-small\", language=\"telugu\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### *WhisperProcessor*"
      ],
      "metadata": {
        "id": "DViWVW6pqslV"
      },
      "id": "DViWVW6pqslV"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d346726",
      "metadata": {
        "id": "9d346726"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperProcessor\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(\"openai/whisper-small\", language=\"telugu\", task=\"transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can write a function to prepare our data ready for the model:\n",
        "1. Load and resample the audio data by calling `batch[\"audio\"]`\n",
        "2. Use the feature extractor to compute the log-Mel spectrogram input features from our 1-dimensional audio array.\n",
        "3. Encode the transcriptions to label ids through the use of the tokenizer."
      ],
      "metadata": {
        "id": "NnUbqlLTrGPc"
      },
      "id": "NnUbqlLTrGPc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f565ece",
      "metadata": {
        "id": "9f565ece"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(batch):\n",
        "    #load and resample data to 16khz\n",
        "    audio = batch[\"audio\"]\n",
        "    # compute log-Mel \n",
        "    batch[\"input_features\"] = feature_extractor(audio[\"array\"], sampling_rate=audio[\"sampling_rate\"]).input_features[0]\n",
        "    # encode text to label ids \n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98497e6c",
      "metadata": {
        "id": "98497e6c",
        "outputId": "23985114-cce8-4adc-fe85-d51bb596278e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                             \r"
          ]
        }
      ],
      "source": [
        "train_dataset = train_dataset.map(prepare_dataset, num_proc=1)\n",
        "test_dataset = test_dataset.map(prepare_dataset, num_proc=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and Evaluation"
      ],
      "metadata": {
        "id": "TsoaXLHJsGA0"
      },
      "id": "TsoaXLHJsGA0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a Data Collator"
      ],
      "metadata": {
        "id": "WQ4omHm-sWxj"
      },
      "id": "WQ4omHm-sWxj"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4725d8c2",
      "metadata": {
        "id": "4725d8c2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorSpeechSeq2SeqWithPadding:\n",
        "    processor: Any\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        # split inputs and labels since they have to be of different lengths and need different padding methods\n",
        "        # first treat the audio inputs by simply returning torch tensors\n",
        "        input_features = [{\"input_features\": feature[\"input_features\"]} for feature in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "        # get the tokenized label sequences\n",
        "        label_features = [{\"input_ids\": feature[\"labels\"]} for feature in features]\n",
        "        # pad the labels to max length\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "        # replace padding with -100 to ignore loss correctly\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "        # if bos token is appended in previous tokenization step,\n",
        "        # cut bos token here as it's append later anyways\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e966d981",
      "metadata": {
        "id": "e966d981"
      },
      "outputs": [],
      "source": [
        "data_collator = DataCollatorSpeechSeq2SeqWithPadding(processor=processor)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Metrics Wer(word error rate)"
      ],
      "metadata": {
        "id": "V0BPhnvbshsi"
      },
      "id": "V0BPhnvbshsi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70321887",
      "metadata": {
        "id": "70321887"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"wer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "130a0dac",
      "metadata": {
        "id": "130a0dac"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(pred):\n",
        "    pred_ids = pred.predictions\n",
        "    label_ids = pred.label_ids\n",
        "\n",
        "    # replace -100 with the pad_token_id\n",
        "    label_ids[label_ids == -100] = tokenizer.pad_token_id\n",
        "\n",
        "    # we do not want to group tokens when computing the metrics\n",
        "    pred_str = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(label_ids, skip_special_tokens=True)\n",
        "\n",
        "    wer = 100 * metric.compute(predictions=pred_str, references=label_str)\n",
        "\n",
        "    return {\"wer\": wer}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load a Pre-Trained Checkpoint"
      ],
      "metadata": {
        "id": "baPyXizBs0iv"
      },
      "id": "baPyXizBs0iv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70bfb32d",
      "metadata": {
        "id": "70bfb32d"
      },
      "outputs": [],
      "source": [
        "from transformers import WhisperForConditionalGeneration\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43321fb0",
      "metadata": {
        "id": "43321fb0"
      },
      "outputs": [],
      "source": [
        "model.config.forced_decoder_ids = None\n",
        "model.config.suppress_tokens = []"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Configuration"
      ],
      "metadata": {
        "id": "ts83DiYCs6vE"
      },
      "id": "ts83DiYCs6vE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d3a2db12",
      "metadata": {
        "id": "d3a2db12"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./whisper-small-te\",  # change to a your specfic folder\n",
        "    per_device_train_batch_size=16,\n",
        "    gradient_accumulation_steps=1,  # increase by 2x for every 2x decrease in batch size\n",
        "    learning_rate=1e-5,\n",
        "    warmup_steps=500,\n",
        "    max_steps=4000,\n",
        "    gradient_checkpointing=True,\n",
        "    fp16=True,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=8,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=1000,\n",
        "    eval_steps=1000,\n",
        "    logging_steps=25,\n",
        "    report_to=[\"tensorboard\"],\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"wer\",\n",
        "    greater_is_better=False,\n",
        "    push_to_hub=False,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that the above specified code is version-specific. In case of an error, try installing the following one by one. Otherwise, check for other solutions on [`Seq2SeqTraining`](https://github.com/huggingface/transformers/tree/main/examples/legacy/seq2seq)\n"
      ],
      "metadata": {
        "id": "7lKz-YgbtaHx"
      },
      "id": "7lKz-YgbtaHx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "337f6ed9",
      "metadata": {
        "id": "337f6ed9"
      },
      "outputs": [],
      "source": [
        "# !pip install --upgrade accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fdc4acb",
      "metadata": {
        "id": "3fdc4acb"
      },
      "outputs": [],
      "source": [
        "# !pip uninstall -y transformers accelerate\n",
        "# !pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f9b28c6",
      "metadata": {
        "id": "0f9b28c6"
      },
      "outputs": [],
      "source": [
        "# !pip install pytorch-accelerated"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training"
      ],
      "metadata": {
        "id": "8ZBeN5ImvOcY"
      },
      "id": "8ZBeN5ImvOcY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8fcb741",
      "metadata": {
        "id": "a8fcb741"
      },
      "outputs": [],
      "source": [
        "from transformers import Seq2SeqTrainer\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    tokenizer=processor.feature_extractor,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8037085",
      "metadata": {
        "id": "c8037085"
      },
      "outputs": [],
      "source": [
        "processor.save_pretrained(training_args.output_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0dd43d4",
      "metadata": {
        "id": "a0dd43d4"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "in case of Cuda  `out-of-Memory` error try out\n",
        "\n",
        "1->`per_device_train_batch_size`"
      ],
      "metadata": {
        "id": "7Bb-C6XIvdfp"
      },
      "id": "7Bb-C6XIvdfp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transcription using Gradio"
      ],
      "metadata": {
        "id": "cvovfUp9v9_w"
      },
      "id": "cvovfUp9v9_w"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a0866d6",
      "metadata": {
        "id": "7a0866d6"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "import gradio as gr\n",
        "\n",
        "pipe = pipeline(model=\"--------------\") #path to the model or checkpoint\n",
        "\n",
        "def transcribe(audio):\n",
        "    text = pipe(audio)[\"text\"]\n",
        "    return text\n",
        "\n",
        "iface = gr.Interface(\n",
        "    fn=transcribe, \n",
        "    inputs=gr.Audio(source=\"upload\", type=\"filepath\"), \n",
        "    outputs=\"text\",\n",
        "    title=\"Whisper Small telugu\",\n",
        "    description=\"demo for telugu speech recognition using Whisper small model.\",\n",
        ")\n",
        "\n",
        "iface.launch()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}